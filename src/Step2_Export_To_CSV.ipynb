{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bc63d19",
   "metadata": {},
   "source": [
    "# Strava Exports to CSV Files\n",
    "Now that all of our data is uncompressed, we have to transform all of the seperate files to something that can be loaded into a database. MySQL's `LOAD DATA INFILE` is quite efficient according to the Google AI summary :skull:, so if we are able to transform the data for each table into a CSV file then loading the data should be more straightforward.\n",
    "\n",
    "As we get into going through data for various users it gets more difficult to keep track of what data is supposed to represent and who it belongs to, so I think an object oriented approach might be better. In the end, the strat may be to load all the data for each table into a pandas DataFrame, use the pandas.to_csv() method, and then import the generated CSVs into MySQL.\n",
    "\n",
    "For information and advice on the three filetypes used and their contents, this tutorial was heavily referenced: [Parsing fitness tracker data with Python](https://towardsdatascience.com/parsing-fitness-tracker-data-with-python-a59e7dc17418/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f2a957",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Package Imports, Constants, Global Variable\n",
    "\n",
    "Run this cell to import all the packages we need and define some constants. \n",
    "You'll likely need to install any missing packages to your Python environment\n",
    "with pip or your package manager of choice.\n",
    "'''\n",
    "\n",
    "import os\n",
    "import gpxpy\n",
    "import fitdecode\n",
    "import pandas as pd\n",
    "\n",
    "ACTIVITY_DIR_PATH = '../data/export_activities' # Parent directory of all exports\n",
    "cur_activity_id = 1   # Global counter to properly increment activity_id primary key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbcb145",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activity:\n",
    "\n",
    "  def __init__(self, activity_id: int, user_id: int, filepath: os.PathLike):\n",
    "    self.__activity_id = activity_id\n",
    "    self.__user_id = user_id\n",
    "    self.__activity_summary\n",
    "    self.__points_df\n",
    "    self.__load_from_file(filepath)\n",
    "    \n",
    "    def __load_from_file(self, filepath: os.PathLike):\n",
    "      match filepath.split('.')[-1]:\n",
    "        case 'gpx':\n",
    "          self.__load_from_gpx(filepath)\n",
    "        case 'tcx':\n",
    "          self.__load_from_tcx(filepath)\n",
    "        case 'fit':\n",
    "          self.__load_from_fit(filepath)\n",
    "\n",
    "    # Stubs to be replaced with real file parsing code\n",
    "    def __load_from_gpx(filepath: os.PathLike):\n",
    "      pass\n",
    "\n",
    "    def __load_from_tcx(filepath: os.PathLike):\n",
    "      pass\n",
    "\n",
    "    def __load_from_fit(filepath: os.PathLike):\n",
    "      pass\n",
    "\n",
    "    # Getter methods\n",
    "    def get_summary(self) -> dict:\n",
    "      return self.__activity_summary\n",
    "    \n",
    "    def get_points(self) -> pd.DataFrame:\n",
    "      return self.__points_df\n",
    "\n",
    "\n",
    "class User:\n",
    "\n",
    "  def __init__(self, user_id: int, export_filepath: os.PathLike, name: str):\n",
    "    self.__user_id = user_id\n",
    "    self.__name = name\n",
    "    self.__activities = []\n",
    "    self.__load_all_activities(export_filepath)\n",
    "\n",
    "  def __load_all_activities(self, export_filepath):\n",
    "    files = os.listdir(export_filepath)\n",
    "    for file in files:\n",
    "      self.__activities.append(Activity(cur_activity_id, \n",
    "                                        self.__user_id, \n",
    "                                        os.path.join(export_filepath, file)))\n",
    "      cur_activity_id += 1\n",
    "\n",
    "  # Getters for exporting\n",
    "  def get_activity_summaries(self) -> pd.DataFrame:\n",
    "    activity_summaries = [activity.get_summary() for activity in self.__activities]\n",
    "    return pd.DataFrame(activity_summaries)\n",
    "\n",
    "  def get_activity_points(self) -> pd.DataFrame:\n",
    "    point_dfs = [activity.get_points() for activity in self.__activities]\n",
    "    return pd.concat(point_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f768ae17",
   "metadata": {},
   "source": [
    "The cell below gets some info out of every GPX file and puts it into a pandas DataFrame, and can be used as a starting point for parsing .gpx files. There are some issues with it in the sense that it doesn't link the data to the user at all, which is why I'm leaning towards the OO way up above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01096c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_export_dirs = [os.path.join(ACTIVITY_DIR_PATH, entry) \n",
    "                        for entry \n",
    "                        in os.listdir(ACTIVITY_DIR_PATH) \n",
    "                        if os.path.isdir(os.path.join(ACTIVITY_DIR_PATH, entry))]\n",
    "\n",
    "point_dfs = []\n",
    "activity_dfs = []\n",
    "for dir in activity_export_dirs:\n",
    "    gpx_paths = [os.path.join(dir, entry) \n",
    "                  for entry\n",
    "                  in os.listdir(dir)\n",
    "                  if entry.split('.')[-1] == 'gpx']\n",
    "    for gpx_path in gpx_paths:\n",
    "      with open(gpx_path) as f:\n",
    "        gpx = gpxpy.parse(f)\n",
    "        point_dict = {\n",
    "          'latitude': [],\n",
    "          'longitude': [],\n",
    "          'elevation': [],\n",
    "          'time': []\n",
    "        }\n",
    "        for track in gpx.tracks:\n",
    "          for segment in track.segments:\n",
    "            for point in segment.points:\n",
    "              point_dict['time'].append(point.time)\n",
    "              point_dict['latitude'].append(point.latitude)\n",
    "              point_dict['longitude'].append(point.longitude)\n",
    "              point_dict['elevation'].append(point.elevation)\n",
    "              point.dis\n",
    "        df = pd.DataFrame(point_dict)\n",
    "        point_dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41a9066e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    latitude  longitude  elevation                      time\n",
      "0  12.536197 -70.057429        5.2 2022-04-17 14:16:52+00:00\n",
      "1  12.536202 -70.057520        5.3 2022-04-17 14:16:55+00:00\n",
      "2  12.536173 -70.057601        5.8 2022-04-17 14:16:57+00:00\n",
      "3  12.536177 -70.057652        5.8 2022-04-17 14:16:59+00:00\n",
      "4  12.536251 -70.057729        5.7 2022-04-17 14:17:02+00:00\n",
      "1610\n"
     ]
    }
   ],
   "source": [
    "print(point_dfs[0].head())\n",
    "print(len(point_dfs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
