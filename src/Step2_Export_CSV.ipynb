{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bc63d19",
   "metadata": {},
   "source": [
    "# Strava Exports to CSV Files\n",
    "Now that all of our data is uncompressed, we have to transform all of the seperate files to something that can be loaded into a database. MySQL's `LOAD DATA INFILE` is quite efficient for bulk loading, so if we are able to transform the data for each table into a CSV file then loading the data should be more straightforward.\n",
    "\n",
    "As we get into going through data for various users it gets more difficult to keep track of what data is supposed to represent and who it belongs to, so I think an object oriented approach might be better. In the end, the strat may be to load all the data for each table into a pandas DataFrame, use the `pandas.to_csv()` method, and then import the generated CSVs into MySQL.\n",
    "\n",
    "For information and advice on the three filetypes used and their contents, this tutorial was heavily referenced: [Parsing fitness tracker data with Python](https://towardsdatascience.com/parsing-fitness-tracker-data-with-python-a59e7dc17418/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08f2a957",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Package Imports, Constants, Global Variable\n",
    "\n",
    "Run this cell to import all the packages we need and define some constants. \n",
    "You'll likely need to install any missing packages to your Python environment\n",
    "with pip or your package manager of choice.\n",
    "'''\n",
    "\n",
    "import os\n",
    "import gpxpy\n",
    "import fitdecode\n",
    "import pandas as pd\n",
    "import csv\n",
    "from collections import namedtuple\n",
    "import random\n",
    "\n",
    "DATA_DIR_PATH = os.path.join('..', 'data')  # Path of data directory relative to this Jupyter Notebook\n",
    "ACTIVITY_DIR_PATH = os.path.join(DATA_DIR_PATH, 'export_activities') # Parent directory of all exports\n",
    "OUTPUT_DIR_PATH = os.path.join(DATA_DIR_PATH, 'outputs')\n",
    "cur_activity_id = 0   # Global activity counter to give each activity a unique id across users\n",
    "MYSQL_NULL = 'NULL'\n",
    "\n",
    "# Namedtuple to pass around fields we want to end up in the activities table\n",
    "Activity_Fields = namedtuple('Activity_Fields', ['user_id', \n",
    "                                                 'activity_id', \n",
    "                                                 'name', \n",
    "                                                 'type', \n",
    "                                                 'description', \n",
    "                                                 'filename'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73f73bd",
   "metadata": {},
   "source": [
    "Let's define an `Activity` object. We will use this object to store all the data from an individual file from an export, whether it is a .fit, .gpx, or .tcx file. By feeding the path to the activity file in the constructor, we are able to make an `Activity` create itself from a file when it is instantiated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ebbcb145",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activity:\n",
    "\n",
    "  __mysql_null = 'NULL'\n",
    "  __activity_summary_keys = ['start_datetime', 'end_datetime', \n",
    "                             'distance_2d', 'distance_3d',\n",
    "                             'avg_speed', 'max_speed',\n",
    "                             'uphill', 'downhill',\n",
    "                             'avg_hr', 'min_hr', 'max_hr',\n",
    "                             'avg_cad','min_cad','max_cad',\n",
    "                             'total_kcal']\n",
    "\n",
    "\n",
    "  def __init__(self, activity_fields: Activity_Fields, activity_filepath: os.PathLike):\n",
    "    self.__activity_id = activity_fields.activity_id\n",
    "    self.__activity_filepath = activity_filepath\n",
    "    self.__points_df = pd.DataFrame()\n",
    "\n",
    "    self.__activity_summary = activity_fields._asdict()\n",
    "    \n",
    "    global MYSQL_NULL\n",
    "    self.__activity_summary.update(dict.fromkeys(self.__activity_summary_keys,\n",
    "                                                 MYSQL_NULL))\n",
    "\n",
    "    self.__point_dict = {\n",
    "      'activity_id': [],\n",
    "      'latitude': [],\n",
    "      'longitude': [],\n",
    "      'elevation': [],\n",
    "      'time': [],\n",
    "      'speed': [],\n",
    "      'hr': [],\n",
    "      'cad': []\n",
    "    }\n",
    "\n",
    "    self.__load_from_file()\n",
    "    \n",
    "\n",
    "  def __load_from_file(self) -> None:\n",
    "    match self.__activity_filepath.split('.')[-1].lower():\n",
    "      case 'gpx':\n",
    "        self.__load_from_gpx()\n",
    "      case 'tcx':\n",
    "        self.__load_from_tcx()\n",
    "      case 'fit':\n",
    "        self.__load_from_fit()\n",
    "\n",
    "\n",
    "  def __load_from_gpx(self) -> None:\n",
    "\n",
    "    with open(self.__activity_filepath) as f:\n",
    "      gpx = gpxpy.parse(f)\n",
    "\n",
    "      uphill, downhill = 0, 0\n",
    "\n",
    "      if len(gpx.tracks) == 0:\n",
    "        raise ValueError(f'No tracks found in gpx file {os.path.abspath(self.__activity_filepath)}')\n",
    "\n",
    "      for track in gpx.tracks:\n",
    "\n",
    "        uphill_downhill = track.get_uphill_downhill()\n",
    "        uphill += uphill_downhill.uphill\n",
    "        downhill += uphill_downhill.downhill\n",
    "        \n",
    "        for segment in track.segments:\n",
    "          for point_idx, point in enumerate(segment.points):            \n",
    "            self.__point_dict['activity_id'].append(self.__activity_id)\n",
    "            self.__point_dict['time'].append(point.time)\n",
    "            self.__point_dict['latitude'].append(point.latitude)\n",
    "            self.__point_dict['longitude'].append(point.longitude)\n",
    "            self.__point_dict['elevation'].append(point.elevation)\n",
    "\n",
    "            # Adding speed\n",
    "            point_speed = point.speed\n",
    "            if point_idx == 0:\n",
    "              point_speed = 0\n",
    "            elif point_speed == None:\n",
    "              point_speed = point.speed_between(segment.points[point_idx - 1])\n",
    "            self.__point_dict['speed'].append(round(point_speed, 3))\n",
    "\n",
    "            # Adding extensions\n",
    "            found_hr = False\n",
    "            found_cad = False\n",
    "            for extension in point.extensions:\n",
    "\n",
    "              hr_element = extension.find('{http://www.garmin.com/xmlschemas/TrackPointExtension/v1}hr')\n",
    "              cad_element = extension.find('{http://www.garmin.com/xmlschemas/TrackPointExtension/v1}cad')\n",
    "              \n",
    "              # Adding heart rate, if exists\n",
    "              if hr_element != None and hr_element.text:\n",
    "                self.__point_dict['hr'].append(int(hr_element.text))\n",
    "                found_hr = True\n",
    "              \n",
    "              # Adding cadence, if exists\n",
    "              if cad_element != None and cad_element.text:\n",
    "                self.__point_dict['cad'].append(int(cad_element.text))\n",
    "                found_cad = True\n",
    "            \n",
    "            # Adding nulls if cadence or heart rate don't exist\n",
    "            if not found_hr:\n",
    "              self.__point_dict['hr'].append('\\\\N')\n",
    "            if not found_cad:\n",
    "              self.__point_dict['cad'].append('\\\\N')\n",
    "      \n",
    "      # Creating the dataframe of points from the dictionary\n",
    "      self.__points_df = pd.DataFrame(self.__point_dict)\n",
    "\n",
    "      # Populating activity summary dictionary\n",
    "      timebounds = gpx.get_time_bounds()\n",
    "      self.__activity_summary.update({'start_datetime': timebounds.start_time,\n",
    "                                      'end_datetime': timebounds.end_time,\n",
    "                                      'distance_2d': round(gpx.length_2d(), 3),\n",
    "                                      'distance_3d': round(gpx.length_3d(), 3),\n",
    "                                      'avg_speed': round(self.__points_df['speed'].mean(), 3),\n",
    "                                      'max_speed': round(self.__points_df['speed'].max(), 3),\n",
    "                                      'uphill': round(uphill, 3),\n",
    "                                      'downhill': round(downhill, 3)})\n",
    "      \n",
    "      questionable_cols = ['hr', 'cad']\n",
    "      for col in questionable_cols:\n",
    "        if(self.__points_df[col].dtype == 'int64'):\n",
    "          self.__activity_summary.update({'avg_' + col: round(self.__points_df[col].mean(), 3),\n",
    "                                          'min_' + col: self.__points_df[col].min(),\n",
    "                                          'max_' + col: self.__points_df[col].max()})\n",
    "    \n",
    "\n",
    "    # Ensure activity_id is stored as integer\n",
    "    self.__points_df['activity_id'] = self.__points_df['activity_id'].astype(int)\n",
    "\n",
    "\n",
    "  # Stubs to be replaced with real file parsing code\n",
    "  def __load_from_tcx(self) -> None:\n",
    "    pass\n",
    "\n",
    "\n",
    "  def __load_from_fit(self) -> None:\n",
    "    pass\n",
    "\n",
    "\n",
    "  # Getter methods\n",
    "  def get_summary(self) -> dict:\n",
    "    return self.__activity_summary\n",
    "  \n",
    "\n",
    "  def get_points(self) -> pd.DataFrame:\n",
    "    return self.__points_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5debbcf",
   "metadata": {},
   "source": [
    "Now let's define a `User`. An entire export directory of activity files belongs to a Strava user, so our `User` can have a list of `Activities`. By feeding the path to the export directory into the constructor, a `User` is able to initialize itself with all of its `Activities` upon instantiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "44c4315f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class User:\n",
    "\n",
    "  __user_field_keys = ['user_id', \n",
    "                       'email_address', \n",
    "                       'first_name', \n",
    "                       'last_name', \n",
    "                       'description', \n",
    "                       'weight', \n",
    "                       'city', \n",
    "                       'state', \n",
    "                       'country']\n",
    "\n",
    "\n",
    "  def __init__(self, export_filepath: os.PathLike):\n",
    "    self.__export_filepath = export_filepath\n",
    "\n",
    "    self.__user_id = 0\n",
    "    self.__user_fields = {}\n",
    "    self.__load_user_info( anonymized=True)\n",
    "\n",
    "    self.__activities = []\n",
    "    self.__load_all_activities()\n",
    "\n",
    "    self.__load_challenges()\n",
    "\n",
    "    self.__load_follows()\n",
    "\n",
    "\n",
    "  def __load_user_info(self, anonymized = False):\n",
    "    profile_csv = csv.DictReader(open(os.path.join(self.__export_filepath, 'profile.csv'), encoding = 'utf-8'))\n",
    "    for row in profile_csv:    \n",
    "      if anonymized:\n",
    "        self.__user_fields = dict.fromkeys(self.__user_field_keys, 'sample_data (anonymized)')\n",
    "      else:\n",
    "        global MYSQL_NULL\n",
    "        self.__user_fields = dict.fromkeys(self.__user_field_keys, MYSQL_NULL)\n",
    "        for key in self.__user_field_keys[1:]:\n",
    "          colname = ' '.join(key.split('_')).title()\n",
    "          self.__user_fields[key] = row[colname] if row[colname] != '' else MYSQL_NULL\n",
    "\n",
    "      self.__user_id = self.__user_fields['user_id'] = row['Athlete ID']\n",
    "      break\n",
    "\n",
    "\n",
    "  def __load_all_activities(self):\n",
    "    # Read in activities.csv for this User\n",
    "    activity_csv_df = pd.read_csv(os.path.join(self.__export_filepath, 'activities.csv'),\n",
    "                                  usecols=['Activity Name', \n",
    "                                           'Activity Type',\n",
    "                                           'Activity Description',\n",
    "                                           'Filename'])\n",
    "    files = os.listdir(self.__export_filepath)\n",
    "    for file in files:\n",
    "      # Only process known file types\n",
    "      file_ext = file.split('.')[-1].lower()\n",
    "      if file_ext in ['gpx']:#, 'tcx', 'fit']:\n",
    "        try:\n",
    "          global cur_activity_id\n",
    "          df_row = activity_csv_df[activity_csv_df['Filename'].str.contains(file, na=False)]\n",
    "          cur_activity_fields = Activity_Fields(user_id=self.__user_id, \n",
    "                                                activity_id=cur_activity_id,\n",
    "                                                name=df_row['Activity Name'].iloc[0],\n",
    "                                                type=df_row['Activity Type'].iloc[0],\n",
    "                                                description=df_row['Activity Description'].iloc[0],\n",
    "                                                filename=file)\n",
    "          self.__activities.append(Activity(activity_fields=cur_activity_fields,\n",
    "                                            activity_filepath = os.path.join(self.__export_filepath, file)))\n",
    "          cur_activity_id += 1\n",
    "        except ValueError as ve:\n",
    "          print(f'Error occured when loading {file}: {ve}')\n",
    "\n",
    "\n",
    "  def __load_challenges(self):\n",
    "    colnames = ['join_datetime', 'name', 'completed']\n",
    "    global_challenges_csv_df = pd.read_csv(os.path.join(self.__export_filepath, 'global_challenges.csv'),\n",
    "                                           header = 0,\n",
    "                                           names = colnames,\n",
    "                                           parse_dates = ['join_datetime'])\n",
    "    group_challenges_csv_df = pd.read_csv(os.path.join(self.__export_filepath, 'group_challenges.csv'),\n",
    "                                          header = 0,\n",
    "                                          names = colnames,\n",
    "                                          parse_dates = ['join_datetime'])\n",
    "    global_challenges_csv_df['type'] = 'global'\n",
    "    group_challenges_csv_df['type'] = 'group'\n",
    "    challenge_dfs = [global_challenges_csv_df, group_challenges_csv_df]\n",
    "    nonempty_challenge_dfs = [df for df in challenge_dfs if not df.empty]\n",
    "    self.__combined_challenge_df = pd.concat(nonempty_challenge_dfs)\n",
    "    self.__combined_challenge_df['user_id'] = self.__user_id\n",
    "    self.__combined_challenge_df['completed'] = self.__combined_challenge_df['completed'].astype(int)\n",
    "\n",
    "  def __load_follows(self):\n",
    "    colnames = ['follow_status', 'favorite_status']\n",
    "    following_csv_df = pd.read_csv(os.path.join(self.__export_filepath, 'following.csv'),\n",
    "                                   header = 0,\n",
    "                                   names = ['followee_user_id'] + colnames,\n",
    "                                   usecols = ['followee_user_id', 'follow_status'])\n",
    "    following_csv_df['follower_user_id'] = self.__user_id\n",
    "    followers_csv_df = pd.read_csv(os.path.join(self.__export_filepath, 'followers.csv'),\n",
    "                                   header = 0,\n",
    "                                   names = ['follower_user_id'] + colnames,\n",
    "                                   usecols = ['follower_user_id', 'follow_status'])\n",
    "    followers_csv_df['followee_user_id'] = self.__user_id\n",
    "    self.__follow_df = pd.concat([following_csv_df, followers_csv_df])\n",
    "\n",
    "\n",
    "  # Getters for exporting\n",
    "  def get_follows(self) -> pd.DataFrame:\n",
    "    return self.__follow_df\n",
    "\n",
    "  def get_challenges(self) -> pd.DataFrame:\n",
    "    return self.__combined_challenge_df\n",
    "\n",
    "\n",
    "  def get_activity_summaries(self) -> pd.DataFrame:\n",
    "    activity_summaries = [activity.get_summary() for activity in self.__activities]\n",
    "    return pd.DataFrame(activity_summaries)\n",
    "\n",
    "\n",
    "  def get_activity_points(self) -> pd.DataFrame:\n",
    "    point_dfs = [activity.get_points() for activity in self.__activities if not activity.get_points().empty]\n",
    "    return pd.concat(point_dfs)\n",
    "  \n",
    "  \n",
    "  def get_user_fields(self) -> dict:\n",
    "    return self.__user_fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e46798",
   "metadata": {},
   "source": [
    "Keeps track of multiple `Users`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931412e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Secretary:\n",
    "\n",
    "  def __init__(self, all_exports: os.PathLike):\n",
    "    self.__all_exports_path = all_exports\n",
    "    self.__users = []\n",
    "    self.__load_all_users()\n",
    "\n",
    "  def __load_all_users(self):\n",
    "    dirs = os.listdir(self.__all_exports_path)\n",
    "    i = 1\n",
    "    for dir in dirs:\n",
    "      print(f'Loading user {i} of {len(dirs)} ...')\n",
    "      self.__users.append(User(os.path.join(self.__all_exports_path, dir)))\n",
    "      i += 1\n",
    "    print('Done loading in all users!')\n",
    "\n",
    "  def export_all_to_csvs(self, dest_path: os.PathLike):\n",
    "    self.__create_points_csv(os.path.join(dest_path, 'points.csv'))\n",
    "    self.__create_activities_csv(os.path.join(dest_path, 'activities.csv'))\n",
    "    self.__create_users_csv(os.path.join(dest_path, 'users.csv'))\n",
    "    self.__create_challenges_csv(os.path.join(dest_path, 'challenges.csv'))\n",
    "    self.__create_follows_csv(os.path.join(dest_path, 'follows.csv'))\n",
    "\n",
    "  def __create_points_csv(self, dest_path: os.PathLike):\n",
    "    point_dfs = [user.get_activity_points() for user in self.__users if not user.get_activity_points().empty]\n",
    "    combined_points_df = pd.concat(point_dfs)\n",
    "    combined_points_df.to_csv(dest_path, index_label='seq_num', lineterminator='\\n')\n",
    "\n",
    "  def __create_activities_csv(self, dest_path: os.PathLike):\n",
    "    activity_dfs = [user.get_activity_summaries() for user in self.__users if not user.get_activity_summaries().empty]\n",
    "    combined_activities_df = pd.concat(activity_dfs)\n",
    "    combined_activities_df.to_csv(dest_path, index = False, lineterminator='\\n')\n",
    "\n",
    "  def __create_users_csv(self, dest_path: os.PathLike):\n",
    "    all_user_fields = [user.get_user_fields() for user in self.__users]\n",
    "    combined_users_df = pd.DataFrame(all_user_fields)\n",
    "    combined_users_df.to_csv(dest_path, index = False)\n",
    "\n",
    "  def __create_challenges_csv(self, dest_path: os.PathLike):\n",
    "    challenge_dfs = [user.get_challenges() for user in self.__users if not user.get_challenges().empty]\n",
    "    combined_challenges_df = pd.concat(challenge_dfs, ignore_index=True)\n",
    "    combined_challenges_df.to_csv(dest_path, index_label='challenge_id')\n",
    "\n",
    "  def __create_follows_csv(self, dest_path: os.PathLike):\n",
    "    follow_dfs = [user.get_follows() for user in self.__users]\n",
    "    combined_follows_df = pd.concat(follow_dfs)\n",
    "    user_ids = [int(user.get_user_fields()['user_id']) for user in self.__users]\n",
    "    print(user_ids)\n",
    "    print(combined_follows_df)\n",
    "    self.__combined_follows_df = combined_follows_df\n",
    "    self.__combined_follows_df = combined_follows_df[(combined_follows_df['follower_user_id'].isin(user_ids)) & \n",
    "                                                     (combined_follows_df['followee_user_id'].isin(user_ids))]\n",
    "    print(self.__combined_follows_df)\n",
    "    self.__combined_follows_df.to_csv(dest_path, index = False)\n",
    "\n",
    "  def get_follows(self) -> pd.DataFrame:\n",
    "    return self.__combined_follows_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9d50dc",
   "metadata": {},
   "source": [
    "Let's test it out by initializing a `User`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4e1f34e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# steve = User(export_filepath = '../data/export_activities/export_101635319')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "df480cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(steve.get_follows().info())\n",
    "# follows_df = steve.get_follows()\n",
    "# follows_df.to_csv('../data/follows.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dcc5b9",
   "metadata": {},
   "source": [
    "And let's look at the activities summaries CSV for this user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b3873739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# points_df = steve.get_activity_points()\n",
    "# points_df.to_csv('../data/points.csv', index_label='seq_num', lineterminator='\\n')\n",
    "\n",
    "# activities_df = steve.get_activity_summaries()\n",
    "# print(activities_df.head(15))\n",
    "# activities_df.to_csv('../data/activities_summaries.csv', index = False, lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8bc49999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading user 1 of 3 ...\n",
      "Error occured when loading 7077892227.gpx: No tracks found in gpx file c:\\Users\\matth\\Documents\\Python\\CS3200\\CS3200_Strava_Secretary\\data\\export_activities\\export_101635319\\7077892227.gpx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matth\\AppData\\Local\\Temp\\ipykernel_21492\\1958986006.py:75: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  global_challenges_csv_df = pd.read_csv(os.path.join(self.__export_filepath, 'global_challenges.csv'),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading user 2 of 3 ...\n",
      "Loading user 3 of 3 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matth\\AppData\\Local\\Temp\\ipykernel_21492\\1958986006.py:75: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  global_challenges_csv_df = pd.read_csv(os.path.join(self.__export_filepath, 'global_challenges.csv'),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done loading in all users!\n"
     ]
    }
   ],
   "source": [
    "secretary = Secretary(all_exports = os.path.abspath(ACTIVITY_DIR_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "22965faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matth\\AppData\\Local\\Temp\\ipykernel_21492\\1958986006.py:121: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(point_dfs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['101635319', '57141745', '96589216']\n",
      "    followee_user_id follow_status follower_user_id\n",
      "0           36064991      Accepted        101635319\n",
      "1           41868570      Accepted        101635319\n",
      "2           45050667      Accepted        101635319\n",
      "3           45711813      Accepted        101635319\n",
      "4           47388563      Accepted        101635319\n",
      "..               ...           ...              ...\n",
      "130         96589216      Accepted        148511532\n",
      "131         96589216      Accepted        150679523\n",
      "132         96589216      Accepted        153835805\n",
      "133         96589216      Accepted        158514221\n",
      "134         96589216      Accepted        159639266\n",
      "\n",
      "[903 rows x 3 columns]\n",
      "    followee_user_id follow_status follower_user_id\n",
      "0           36064991      Accepted        101635319\n",
      "1           41868570      Accepted        101635319\n",
      "2           45050667      Accepted        101635319\n",
      "3           45711813      Accepted        101635319\n",
      "4           47388563      Accepted        101635319\n",
      "..               ...           ...              ...\n",
      "130         96589216      Accepted        148511532\n",
      "131         96589216      Accepted        150679523\n",
      "132         96589216      Accepted        153835805\n",
      "133         96589216      Accepted        158514221\n",
      "134         96589216      Accepted        159639266\n",
      "\n",
      "[903 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# print(os.path.exists('../data/export_activities'))\n",
    "# print(ACTIVITY_DIR_PATH)\n",
    "os.makedirs(OUTPUT_DIR_PATH, exist_ok=True)\n",
    "secretary.export_all_to_csvs(os.path.join('..', 'data', 'outputs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "186ae487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101635319, 57141745, 96589216]\n",
      "    followee_user_id follow_status follower_user_id\n",
      "10          57141745      Accepted        101635319\n",
      "23          96589216      Accepted        101635319\n",
      "206         96589216      Accepted         57141745\n",
      "212        101635319      Accepted         57141745\n",
      "42          57141745      Accepted         96589216\n",
      "73         101635319      Accepted         96589216\n",
      "Empty DataFrame\n",
      "Columns: [followee_user_id, follow_status, follower_user_id]\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matth\\AppData\\Local\\Temp\\ipykernel_21492\\4122973669.py:8: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  combined_follows_df = combined_follows_df[(follows['follower_user_id'].isin(user_ids))]\n"
     ]
    }
   ],
   "source": [
    "follows = secretary.get_follows()\n",
    "\n",
    "user_ids = ['101635319', '57141745', '96589216']\n",
    "user_ids = [int(id) for id in user_ids]\n",
    "print(user_ids)\n",
    "combined_follows_df = follows[(follows['followee_user_id'].isin(user_ids))]\n",
    "print(combined_follows_df)\n",
    "combined_follows_df = combined_follows_df[(follows['follower_user_id'].isin(user_ids))]\n",
    "print(combined_follows_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
