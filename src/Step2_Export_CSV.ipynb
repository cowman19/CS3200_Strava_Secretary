{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bc63d19",
   "metadata": {},
   "source": [
    "# Strava Exports to CSV Files\n",
    "Now that all of our data is uncompressed, we have to transform all of the seperate files to something that can be loaded into a database. MySQL's `LOAD DATA INFILE` is quite efficient for bulk loading, so if we are able to transform the data for each table into a CSV file then loading the data should be more straightforward.\n",
    "\n",
    "As we get into going through data for various users it gets more difficult to keep track of what data is supposed to represent and who it belongs to, so I think an object oriented approach might be better. In the end, we want load all the data for each table into a pandas DataFrame, use the `pandas.to_csv()` method, and then import the generated CSVs into MySQL.\n",
    "\n",
    "For information and advice on the three filetypes used and their contents, this tutorial was heavily referenced: [Parsing fitness tracker data with Python](https://towardsdatascience.com/parsing-fitness-tracker-data-with-python-a59e7dc17418/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "08f2a957",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Package Imports, Constants, Global Variable\n",
    "\n",
    "Run this cell to import all the packages we need and define some constants. \n",
    "You'll likely need to install any missing packages to your Python environment\n",
    "with pip or your package manager of choice.\n",
    "'''\n",
    "\n",
    "import os\n",
    "import gpxpy\n",
    "import fitdecode\n",
    "import pandas as pd\n",
    "import csv\n",
    "from collections import namedtuple\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "DATA_DIR_PATH = os.path.join('..', 'data')  # Path of data directory relative to this Jupyter Notebook\n",
    "ACTIVITY_DIR_PATH = os.path.join(DATA_DIR_PATH, 'export_activities') # Parent directory of all exports\n",
    "OUTPUT_DIR_PATH = os.path.join(DATA_DIR_PATH, 'outputs')\n",
    "cur_activity_id = 0   # Global activity counter to give each activity a unique id across users\n",
    "MYSQL_NULL = 'NULL'\n",
    "\n",
    "# Namedtuple to pass around fields we want to end up in the activities table\n",
    "Activity_Fields = namedtuple('Activity_Fields', ['user_id', \n",
    "                                                 'activity_id', \n",
    "                                                 'name', \n",
    "                                                 'type', \n",
    "                                                 'description', \n",
    "                                                 'filename'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73f73bd",
   "metadata": {},
   "source": [
    "Let's define an `Activity` object. We will use this object to store all the data from an individual file from an export, whether it is a .fit, .gpx, or .tcx file. By feeding the path to the activity file in the constructor, we are able to make an `Activity` create itself from a file when it is instantiated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbcb145",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activity:\n",
    "\n",
    "  __activity_summary_keys = ['start_datetime', 'end_datetime', \n",
    "                             'distance_2d', 'distance_3d',\n",
    "                             'avg_speed', 'max_speed',\n",
    "                             'uphill', 'downhill',\n",
    "                             'avg_hr', 'min_hr', 'max_hr',\n",
    "                             'avg_cad','min_cad','max_cad',\n",
    "                             'total_kcal']\n",
    "\n",
    "\n",
    "  def __init__(self, activity_fields: Activity_Fields, activity_filepath: os.PathLike):\n",
    "    self.__activity_id = activity_fields.activity_id\n",
    "    self.__activity_filepath = activity_filepath\n",
    "    self.__points_df = pd.DataFrame()\n",
    "\n",
    "    self.__activity_summary = activity_fields._asdict()\n",
    "    \n",
    "    global MYSQL_NULL\n",
    "    self.__activity_summary.update(dict.fromkeys(self.__activity_summary_keys,\n",
    "                                                 MYSQL_NULL))\n",
    "\n",
    "    self.__point_dict = {\n",
    "      'activity_id': [],\n",
    "      'latitude': [],\n",
    "      'longitude': [],\n",
    "      'elevation': [],\n",
    "      'time': [],\n",
    "      'speed': [],\n",
    "      'hr': [],\n",
    "      'cad': []\n",
    "    }\n",
    "\n",
    "    self.__load_from_file()\n",
    "    \n",
    "\n",
    "  def __load_from_file(self) -> None:\n",
    "    match self.__activity_filepath.split('.')[-1].lower():\n",
    "      case 'gpx':\n",
    "        self.__load_from_gpx()\n",
    "      case 'tcx':\n",
    "        self.__load_from_tcx()\n",
    "      case 'fit':\n",
    "        self.__load_from_fit()\n",
    "\n",
    "\n",
    "  def __load_from_gpx(self) -> None:\n",
    "\n",
    "    with open(self.__activity_filepath) as f:\n",
    "      gpx = gpxpy.parse(f)\n",
    "\n",
    "      uphill, downhill = 0, 0\n",
    "\n",
    "      if len(gpx.tracks) == 0:\n",
    "        raise ValueError(f'No tracks found in gpx file {os.path.abspath(self.__activity_filepath)}')\n",
    "\n",
    "      for track in gpx.tracks:\n",
    "\n",
    "        uphill_downhill = track.get_uphill_downhill()\n",
    "        uphill += uphill_downhill.uphill\n",
    "        downhill += uphill_downhill.downhill\n",
    "        \n",
    "        for segment in track.segments:\n",
    "          for point_idx, point in enumerate(segment.points):            \n",
    "            self.__point_dict['activity_id'].append(self.__activity_id)\n",
    "            self.__point_dict['time'].append(point.time)\n",
    "            self.__point_dict['latitude'].append(point.latitude)\n",
    "            self.__point_dict['longitude'].append(point.longitude)\n",
    "            self.__point_dict['elevation'].append(point.elevation)\n",
    "\n",
    "            # Adding speed\n",
    "            point_speed = point.speed\n",
    "            if point_idx == 0:\n",
    "              point_speed = 0\n",
    "            elif point_speed == None:\n",
    "              point_speed = point.speed_between(segment.points[point_idx - 1])\n",
    "            self.__point_dict['speed'].append(round(point_speed, 3))\n",
    "\n",
    "            # Adding extensions\n",
    "            found_hr = False\n",
    "            found_cad = False\n",
    "            for extension in point.extensions:\n",
    "\n",
    "              hr_element = extension.find('{http://www.garmin.com/xmlschemas/TrackPointExtension/v1}hr')\n",
    "              cad_element = extension.find('{http://www.garmin.com/xmlschemas/TrackPointExtension/v1}cad')\n",
    "              \n",
    "              # Adding heart rate, if exists\n",
    "              if hr_element != None and hr_element.text:\n",
    "                self.__point_dict['hr'].append(int(hr_element.text))\n",
    "                found_hr = True\n",
    "              \n",
    "              # Adding cadence, if exists\n",
    "              if cad_element != None and cad_element.text:\n",
    "                self.__point_dict['cad'].append(int(cad_element.text))\n",
    "                found_cad = True\n",
    "            \n",
    "            global MYSQL_NULL\n",
    "            # Adding nulls if cadence or heart rate don't exist\n",
    "            if not found_hr:\n",
    "              self.__point_dict['hr'].append(MYSQL_NULL)\n",
    "            if not found_cad:\n",
    "              self.__point_dict['cad'].append(MYSQL_NULL)\n",
    "      \n",
    "      # Creating the dataframe of points from the dictionary\n",
    "      self.__points_df = pd.DataFrame(self.__point_dict)\n",
    "\n",
    "      # Populating activity summary dictionary\n",
    "      timebounds = gpx.get_time_bounds()\n",
    "      self.__activity_summary.update({'start_datetime': timebounds.start_time,\n",
    "                                      'end_datetime': timebounds.end_time,\n",
    "                                      'distance_2d': round(gpx.length_2d(), 3),\n",
    "                                      'distance_3d': round(gpx.length_3d(), 3),\n",
    "                                      'avg_speed': round(self.__points_df['speed'].mean(), 3),\n",
    "                                      'max_speed': round(self.__points_df['speed'].max(), 3),\n",
    "                                      'uphill': round(uphill, 3),\n",
    "                                      'downhill': round(downhill, 3)})\n",
    "      \n",
    "      questionable_cols = ['hr', 'cad']\n",
    "      for col in questionable_cols:\n",
    "        if(self.__points_df[col].dtype == 'int64'):\n",
    "          self.__activity_summary.update({'avg_' + col: round(self.__points_df[col].mean(), 3),\n",
    "                                          'min_' + col: self.__points_df[col].min(),\n",
    "                                          'max_' + col: self.__points_df[col].max()})\n",
    "    \n",
    "\n",
    "    # Ensure activity_id is stored as integer\n",
    "    self.__points_df['activity_id'] = self.__points_df['activity_id'].astype(int)\n",
    "\n",
    "\n",
    "  # Stubs to be replaced with real file parsing code\n",
    "  def __load_from_tcx(self) -> None:\n",
    "    pass\n",
    "\n",
    "  # Helper for __load_from_fit\n",
    "  def __convert_semicircles_to_degrees(self, val):\n",
    "      if val is None:\n",
    "        return None\n",
    "      try:\n",
    "        # Some fit decoders return semicircles; convert only when value appears large\n",
    "        if abs(val) > 90:\n",
    "          return val * (180.0 / (2**31))\n",
    "        return val\n",
    "      except Exception:\n",
    "        return val\n",
    "\n",
    "  # The following function is an absolute mess and desperately needs a refactor :(\n",
    "  def __load_from_fit(self) -> None:\n",
    "    # Parse .fit file using fitdecode\n",
    "    # Use session messages to populate activity summary and record messages to populate points\n",
    "\n",
    "    # iterate through fit file messages\n",
    "    with fitdecode.FitReader(self.__activity_filepath) as fit:\n",
    "\n",
    "      current_speed = 0.0\n",
    "      current_altitude = 0.0\n",
    "\n",
    "      for frame in fit:\n",
    "        global MYSQL_NULL\n",
    "        # only process data messages\n",
    "        if not isinstance(frame, fitdecode.records.FitDataMessage):\n",
    "          continue\n",
    "          \n",
    "        # session messages -> summary fields. Assuming one session per fit file\n",
    "        if frame.name == 'session':\n",
    "          try:\n",
    "            start_time = frame.get_value('start_time')\n",
    "            duration = frame.get_value('total_elapsed_time')\n",
    "            elapsed_timedelta = timedelta(seconds = duration)\n",
    "            end_time = start_time + elapsed_timedelta\n",
    "            total_distance = frame.get_value('total_distance')\n",
    "          except Exception as e:\n",
    "            raise ValueError(f'Unable to find time bounds or total distance in fit file session {os.path.abspath(self.__activity_filepath)}')\n",
    "          \n",
    "          total_calories = frame.get_value('total_calories')\n",
    "\n",
    "          avg_speed, max_speed = 0,0\n",
    "          try:\n",
    "            avg_speed = frame.get_value('avg_speed')\n",
    "            max_speed = frame.get_value('max_speed')\n",
    "          except KeyError:\n",
    "            pass\n",
    "          \n",
    "\n",
    "          # elevation gain/descent fields may have different names across devices\n",
    "          total_ascent = 0\n",
    "          try:\n",
    "            total_ascent = frame.get_value('total_ascent')\n",
    "          except KeyError:\n",
    "            pass\n",
    "          try:\n",
    "            total_ascent = frame.get_value('total_elevation_gain')\n",
    "          except KeyError:\n",
    "            pass\n",
    "\n",
    "          total_descent = 0\n",
    "          try:\n",
    "            total_descent = frame.get_value('total_descent')\n",
    "          except KeyError:\n",
    "            pass\n",
    "          try:\n",
    "            total_descent = frame.get_value('total_elevation_loss')\n",
    "          except KeyError:\n",
    "            pass\n",
    "\n",
    "          uphill, downhill = total_ascent, total_descent\n",
    "\n",
    "          self.__activity_summary.update({'start_datetime': start_time,\n",
    "                                      'end_datetime': end_time,\n",
    "                                      'distance_2d': round(total_distance, 3),\n",
    "                                      'avg_speed': round(avg_speed, 3) if avg_speed else MYSQL_NULL,\n",
    "                                      'max_speed': round(max_speed, 3) if max_speed else MYSQL_NULL,\n",
    "                                      'uphill': round(uphill, 3) if uphill else MYSQL_NULL,\n",
    "                                      'downhill': round(downhill, 3) if downhill else MYSQL_NULL,\n",
    "                                      'total_kcal': int(total_calories)})\n",
    "\n",
    "        # Using gps_metadata messages to substitute for when altitude is not available\n",
    "        # in record frame\n",
    "        elif frame.name == 'gps_metadata':\n",
    "          altitude = current_altitude\n",
    "\n",
    "          try:\n",
    "            interim_altitude = frame.get_value('altitude')\n",
    "            if interim_altitude:\n",
    "              altitude = interim_altitude\n",
    "          except KeyError:\n",
    "            pass\n",
    "          \n",
    "          try:\n",
    "            interim_altitude = frame.get_value('enhanced_altitude')\n",
    "            if interim_altitude:\n",
    "              altitude = interim_altitude\n",
    "          except KeyError:\n",
    "            pass\n",
    "\n",
    "          current_altitude = altitude\n",
    "\n",
    "        # record messages -> per-point data\n",
    "        elif frame.name == 'record':\n",
    "\n",
    "          lat, lon = None, None\n",
    "          try:\n",
    "            lat = frame.get_value('position_lat', raw_value = False)\n",
    "            lon = frame.get_value('position_long', raw_value = False)\n",
    "          except KeyError:\n",
    "            pass\n",
    "\n",
    "          if not lat or not lon:\n",
    "            continue\n",
    "\n",
    "          # attempt conversion if semicircles\n",
    "          lat = self.__convert_semicircles_to_degrees(lat)\n",
    "          lon = self.__convert_semicircles_to_degrees(lon)\n",
    "\n",
    "          self.__point_dict['activity_id'].append(self.__activity_id)\n",
    "\n",
    "          self.__point_dict['latitude'].append(lat)\n",
    "          self.__point_dict['longitude'].append(lon)\n",
    "\n",
    "          timestamp = frame.get_value('timestamp')\n",
    "          self.__point_dict['time'].append(timestamp)\n",
    "\n",
    "          altitude = current_altitude\n",
    "          try:\n",
    "            interim_altitude = frame.get_value('altitude')\n",
    "            if interim_altitude:\n",
    "              altitude = interim_altitude\n",
    "          except KeyError:\n",
    "            pass\n",
    "          try:\n",
    "            interim_altitude = frame.get_value('enhanced_altitude')\n",
    "            if interim_altitude:\n",
    "              altitude = interim_altitude\n",
    "          except KeyError:\n",
    "            pass\n",
    "\n",
    "          if isinstance(altitude, tuple):\n",
    "            altitude = altitude[0]\n",
    "\n",
    "          current_altitude = altitude\n",
    "          self.__point_dict['elevation'].append(round(altitude, 3))\n",
    "\n",
    "          speed = current_speed\n",
    "          try:\n",
    "            speed = frame.get_value('speed')\n",
    "          except KeyError:\n",
    "            pass\n",
    "          try:\n",
    "            speed = frame.get_value('enhanced_speed')\n",
    "          except KeyError:\n",
    "            pass\n",
    "          current_speed = speed\n",
    "          self.__point_dict['speed'].append(speed)\n",
    "          \n",
    "          hr = 0\n",
    "          try:\n",
    "            hr = frame.get_value('heart_rate')\n",
    "          except KeyError:\n",
    "            pass\n",
    "          if hr:\n",
    "            self.__point_dict['hr'].append(int(hr))\n",
    "          else:\n",
    "            self.__point_dict['hr'].append(MYSQL_NULL)\n",
    "          \n",
    "          cad = 0\n",
    "          try:\n",
    "            cad = frame.get_value('cadence')\n",
    "          except KeyError:\n",
    "            pass\n",
    "          if cad:\n",
    "            self.__point_dict['cad'].append(int(cad))\n",
    "          else:\n",
    "            self.__point_dict['cad'].append(MYSQL_NULL)\n",
    "\n",
    "    # build dataframe from accumulated point dict\n",
    "    self.__points_df = pd.DataFrame(self.__point_dict)\n",
    "\n",
    "    # Datetime is messed up sometimes, this is a fix I guess\n",
    "    self.__points_df['time'] = self.__points_df['time'].astype(str)\n",
    "    self.__points_df['time'] = self.__points_df['time'].astype(str)\n",
    "\n",
    "    # compute summary fields from points if missing\n",
    "    if 'avg_speed' in self.__activity_summary_keys and (self.__activity_summary.get('avg_speed') == MYSQL_NULL or self.__activity_summary.get('avg_speed') is None):\n",
    "      if not self.__points_df.empty and 'speed' in self.__points_df.columns:\n",
    "        speeds = pd.to_numeric(self.__points_df['speed'], errors='coerce')\n",
    "        if not speeds.dropna().empty:\n",
    "          self.__activity_summary['avg_speed'] = round(speeds.mean(), 3)\n",
    "          self.__activity_summary['max_speed'] = round(speeds.max(), 3)\n",
    "\n",
    "    # compute heart rate / cadence summaries if available\n",
    "    for col in ['hr', 'cad']:\n",
    "      if col in self.__points_df.columns:\n",
    "        col_numeric = pd.to_numeric(self.__points_df[col], errors='coerce')\n",
    "        if not col_numeric.dropna().empty:\n",
    "          self.__activity_summary['avg_' + col] = round(col_numeric.mean(), 3)\n",
    "          self.__activity_summary['min_' + col] = int(col_numeric.min())\n",
    "          self.__activity_summary['max_' + col] = int(col_numeric.max())\n",
    "\n",
    "    self.__points_df['activity_id'] = self.__points_df['activity_id'].astype(int)\n",
    "\n",
    "\n",
    "  # Getter methods\n",
    "  def get_summary(self) -> dict:\n",
    "    return self.__activity_summary\n",
    "  \n",
    "\n",
    "  def get_points(self) -> pd.DataFrame:\n",
    "    return self.__points_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5debbcf",
   "metadata": {},
   "source": [
    "Now let's define a `User`. An entire export directory of activity files belongs to a Strava user, so our `User` can have a list of `Activities`. By feeding the path to the export directory into the constructor, a `User` is able to initialize itself with all of its `Activities` upon instantiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "44c4315f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class User:\n",
    "\n",
    "  __user_field_keys = ['user_id', \n",
    "                       'email_address', \n",
    "                       'first_name', \n",
    "                       'last_name', \n",
    "                       'description', \n",
    "                       'weight', \n",
    "                       'city', \n",
    "                       'state', \n",
    "                       'country']\n",
    "\n",
    "\n",
    "  def __init__(self, export_filepath: os.PathLike):\n",
    "    self.__export_filepath = export_filepath\n",
    "\n",
    "    self.__user_id = 0\n",
    "    self.__user_fields = {}\n",
    "    self.__load_user_info( anonymized=True)\n",
    "\n",
    "    self.__activities = []\n",
    "    self.__load_all_activities()\n",
    "\n",
    "    self.__load_challenges()\n",
    "\n",
    "    self.__load_follows()\n",
    "\n",
    "\n",
    "  def __load_user_info(self, anonymized = False):\n",
    "    profile_csv = csv.DictReader(open(os.path.join(self.__export_filepath, 'profile.csv'), encoding = 'utf-8'))\n",
    "    for row in profile_csv:    \n",
    "      if anonymized:\n",
    "        self.__user_fields = dict.fromkeys(self.__user_field_keys, 'sample_data (anonymized)')\n",
    "      else:\n",
    "        global MYSQL_NULL\n",
    "        self.__user_fields = dict.fromkeys(self.__user_field_keys, MYSQL_NULL)\n",
    "        for key in self.__user_field_keys[1:]:\n",
    "          colname = ' '.join(key.split('_')).title()\n",
    "          self.__user_fields[key] = row[colname] if row[colname] != '' else MYSQL_NULL\n",
    "\n",
    "      self.__user_id = self.__user_fields['user_id'] = row['Athlete ID']\n",
    "      break\n",
    "\n",
    "\n",
    "  def __load_all_activities(self):\n",
    "    # Read in activities.csv for this User\n",
    "    activity_csv_df = pd.read_csv(os.path.join(self.__export_filepath, 'activities.csv'),\n",
    "                                  usecols=['Activity Name', \n",
    "                                           'Activity Type',\n",
    "                                           'Activity Description',\n",
    "                                           'Filename'])\n",
    "    files = os.listdir(self.__export_filepath)\n",
    "    for file in files:\n",
    "      # Only process known file types\n",
    "      file_ext = file.split('.')[-1].lower()\n",
    "      if file_ext in ['gpx', 'fit']:#, 'tcx', 'fit']:\n",
    "        try:\n",
    "          global cur_activity_id\n",
    "          df_row = activity_csv_df[activity_csv_df['Filename'].str.contains(file, na=False)]\n",
    "          cur_activity_fields = Activity_Fields(user_id=self.__user_id, \n",
    "                                                activity_id=cur_activity_id,\n",
    "                                                name=df_row['Activity Name'].iloc[0],\n",
    "                                                type=df_row['Activity Type'].iloc[0],\n",
    "                                                description=df_row['Activity Description'].iloc[0],\n",
    "                                                filename=file)\n",
    "          self.__activities.append(Activity(activity_fields=cur_activity_fields,\n",
    "                                            activity_filepath = os.path.join(self.__export_filepath, file)))\n",
    "          cur_activity_id += 1\n",
    "        except ValueError as ve:\n",
    "          print(f'Error occured when loading {file}: {ve}')\n",
    "\n",
    "\n",
    "  def __load_challenges(self):\n",
    "    colnames = ['join_datetime', 'name', 'completed']\n",
    "    global_challenges_csv_df = pd.read_csv(os.path.join(self.__export_filepath, 'global_challenges.csv'),\n",
    "                                           header = 0,\n",
    "                                           names = colnames,\n",
    "                                           parse_dates = ['join_datetime'])\n",
    "    group_challenges_csv_df = pd.read_csv(os.path.join(self.__export_filepath, 'group_challenges.csv'),\n",
    "                                          header = 0,\n",
    "                                          names = colnames,\n",
    "                                          parse_dates = ['join_datetime'])\n",
    "    global_challenges_csv_df['type'] = 'global'\n",
    "    group_challenges_csv_df['type'] = 'group'\n",
    "    challenge_dfs = [global_challenges_csv_df, group_challenges_csv_df]\n",
    "    nonempty_challenge_dfs = [df for df in challenge_dfs if not df.empty]\n",
    "    self.__combined_challenge_df = pd.concat(nonempty_challenge_dfs)\n",
    "    self.__combined_challenge_df['user_id'] = self.__user_id\n",
    "    self.__combined_challenge_df['completed'] = self.__combined_challenge_df['completed'].astype(int)\n",
    "\n",
    "  def __load_follows(self):\n",
    "    colnames = ['follow_status', 'favorite_status']\n",
    "    following_csv_df = pd.read_csv(os.path.join(self.__export_filepath, 'following.csv'),\n",
    "                                   header = 0,\n",
    "                                   names = ['followee_user_id'] + colnames,\n",
    "                                   usecols = ['followee_user_id', 'follow_status'])\n",
    "    following_csv_df['follower_user_id'] = self.__user_id\n",
    "    followers_csv_df = pd.read_csv(os.path.join(self.__export_filepath, 'followers.csv'),\n",
    "                                   header = 0,\n",
    "                                   names = ['follower_user_id'] + colnames,\n",
    "                                   usecols = ['follower_user_id', 'follow_status'])\n",
    "    followers_csv_df['followee_user_id'] = self.__user_id\n",
    "    self.__follow_df = pd.concat([following_csv_df, followers_csv_df])\n",
    "\n",
    "\n",
    "  # Getters for exporting\n",
    "  def get_follows(self) -> pd.DataFrame:\n",
    "    return self.__follow_df\n",
    "\n",
    "  def get_challenges(self) -> pd.DataFrame:\n",
    "    return self.__combined_challenge_df\n",
    "\n",
    "\n",
    "  def get_activity_summaries(self) -> pd.DataFrame:\n",
    "    activity_summaries = [activity.get_summary() for activity in self.__activities]\n",
    "    return pd.DataFrame(activity_summaries)\n",
    "\n",
    "\n",
    "  def get_activity_points(self) -> pd.DataFrame:\n",
    "    point_dfs = [activity.get_points() for activity in self.__activities if not activity.get_points().empty]\n",
    "    return pd.concat(point_dfs)\n",
    "  \n",
    "  \n",
    "  def get_user_fields(self) -> dict:\n",
    "    return self.__user_fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e46798",
   "metadata": {},
   "source": [
    "A `Secretary` keeps track of multiple `Users`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "931412e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Secretary:\n",
    "\n",
    "  def __init__(self, all_exports: os.PathLike):\n",
    "    self.__all_exports_path = all_exports\n",
    "    self.__users = []\n",
    "    self.__load_all_users()\n",
    "\n",
    "  def __load_all_users(self):\n",
    "    dirs = os.listdir(self.__all_exports_path)\n",
    "    i = 1\n",
    "    for dir in dirs:\n",
    "      print(f'Loading user {i} of {len(dirs)} ...')\n",
    "      self.__users.append(User(os.path.join(self.__all_exports_path, dir)))\n",
    "      i += 1\n",
    "    print('Done loading in all users!')\n",
    "\n",
    "  def export_all_to_csvs(self, dest_path: os.PathLike):\n",
    "    self.__create_points_csv(os.path.join(dest_path, 'points.csv'))\n",
    "    self.__create_activities_csv(os.path.join(dest_path, 'activities.csv'))\n",
    "    self.__create_users_csv(os.path.join(dest_path, 'users.csv'))\n",
    "    self.__create_challenges_csv(os.path.join(dest_path, 'challenges.csv'))\n",
    "    self.__create_follows_csv(os.path.join(dest_path, 'follow.csv'))\n",
    "\n",
    "  def __create_points_csv(self, dest_path: os.PathLike):\n",
    "    point_dfs = [user.get_activity_points() for user in self.__users if not user.get_activity_points().empty]\n",
    "    combined_points_df = pd.concat(point_dfs)\n",
    "    combined_points_df.to_csv(dest_path, index_label='seq_num', lineterminator='\\n')\n",
    "\n",
    "  def __create_activities_csv(self, dest_path: os.PathLike):\n",
    "    activity_dfs = [user.get_activity_summaries() for user in self.__users if not user.get_activity_summaries().empty]\n",
    "    combined_activities_df = pd.concat(activity_dfs)\n",
    "    combined_activities_df.to_csv(dest_path, index = False, lineterminator='\\n')\n",
    "\n",
    "  def __create_users_csv(self, dest_path: os.PathLike):\n",
    "    all_user_fields = [user.get_user_fields() for user in self.__users]\n",
    "    combined_users_df = pd.DataFrame(all_user_fields)\n",
    "    combined_users_df.to_csv(dest_path, index = False)\n",
    "\n",
    "  def __create_challenges_csv(self, dest_path: os.PathLike):\n",
    "    challenge_dfs = [user.get_challenges() for user in self.__users if not user.get_challenges().empty]\n",
    "    combined_challenges_df = pd.concat(challenge_dfs, ignore_index=True)\n",
    "    combined_challenges_df.to_csv(dest_path, index_label='challenge_id')\n",
    "\n",
    "  def __create_follows_csv(self, dest_path: os.PathLike):\n",
    "    follow_dfs = [user.get_follows() for user in self.__users]\n",
    "    combined_follows_df = pd.concat(follow_dfs, ignore_index=True)\n",
    "\n",
    "    # Ensure user IDs are integers and drop rows with NaN values\n",
    "    combined_follows_df['follower_user_id'] = pd.to_numeric(combined_follows_df['follower_user_id'], errors='coerce')\n",
    "    combined_follows_df['followee_user_id'] = pd.to_numeric(combined_follows_df['followee_user_id'], errors='coerce')\n",
    "    combined_follows_df.dropna(subset=['follower_user_id', 'followee_user_id'], inplace=True)\n",
    "\n",
    "    # Convert to integers after dropping NaNs\n",
    "    combined_follows_df['follower_user_id'] = combined_follows_df['follower_user_id'].astype(int)\n",
    "    combined_follows_df['followee_user_id'] = combined_follows_df['followee_user_id'].astype(int)\n",
    "\n",
    "    user_ids = [int(user.get_user_fields()['user_id']) for user in self.__users]\n",
    "    print(\"User IDs:\", user_ids)\n",
    "    print(\"Combined Follows DataFrame (Before Filtering):\\n\", combined_follows_df)\n",
    "\n",
    "    # Filter rows where both follower and followee are in the user_ids list\n",
    "    self.__combined_follows_df = combined_follows_df[\n",
    "      combined_follows_df['follower_user_id'].isin(user_ids) &\n",
    "      combined_follows_df['followee_user_id'].isin(user_ids)\n",
    "    ]\n",
    "\n",
    "    print(\"Filtered Follows DataFrame:\\n\", self.__combined_follows_df)\n",
    "    self.__combined_follows_df.to_csv(dest_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9d50dc",
   "metadata": {},
   "source": [
    "Let's test it out by initializing a `User`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4e1f34e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# steve = User(export_filepath = '../data/export_activities/export_96589216')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9c444c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pts = steve.get_activity_points()\n",
    "# act = steve.get_activity_summaries()\n",
    "\n",
    "# print(pts)\n",
    "\n",
    "# print(act)\n",
    "# act.to_csv('../data/act.csv', index = False)\n",
    "# pts.to_csv('../data/pts.csv', index_label='seq_num', lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "df480cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(steve.get_follows().info())\n",
    "# follows_df = steve.get_follows()\n",
    "# follows_df.to_csv('../data/follows.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dcc5b9",
   "metadata": {},
   "source": [
    "And let's look at the activities summaries CSV for this user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b3873739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# points_df = steve.get_activity_points()\n",
    "# points_df.to_csv('../data/points.csv', index_label='seq_num', lineterminator='\\n')\n",
    "\n",
    "# activities_df = steve.get_activity_summaries()\n",
    "# print(activities_df.head(15))\n",
    "# activities_df.to_csv('../data/activities_summaries.csv', index = False, lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d95a54",
   "metadata": {},
   "source": [
    "A `Secretary` object stores all the `Users` so initializing the following object will load all `Users` and `Activities` in the specified export directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8bc49999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading user 1 of 4 ...\n",
      "Error occured when loading 13902526382.fit: Unable to find time bounds or total distance in fit file session c:\\Users\\matth\\Documents\\Python\\CS3200\\CS3200_Strava_Secretary\\data\\export_activities\\export_101635319\\13902526382.fit\n",
      "Error occured when loading 7077892227.gpx: No tracks found in gpx file c:\\Users\\matth\\Documents\\Python\\CS3200\\CS3200_Strava_Secretary\\data\\export_activities\\export_101635319\\7077892227.gpx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matth\\AppData\\Local\\Temp\\ipykernel_19028\\2876865893.py:75: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  global_challenges_csv_df = pd.read_csv(os.path.join(self.__export_filepath, 'global_challenges.csv'),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading user 2 of 4 ...\n",
      "Error occured when loading 17205460653.fit: Unable to find time bounds or total distance in fit file session c:\\Users\\matth\\Documents\\Python\\CS3200\\CS3200_Strava_Secretary\\data\\export_activities\\export_148511532\\17205460653.fit\n",
      "Error occured when loading 17249570840.fit: Unable to find time bounds or total distance in fit file session c:\\Users\\matth\\Documents\\Python\\CS3200\\CS3200_Strava_Secretary\\data\\export_activities\\export_148511532\\17249570840.fit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matth\\AppData\\Local\\Temp\\ipykernel_19028\\2876865893.py:75: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  global_challenges_csv_df = pd.read_csv(os.path.join(self.__export_filepath, 'global_challenges.csv'),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading user 3 of 4 ...\n",
      "Loading user 4 of 4 ...\n",
      "Error occured when loading 7968742010.fit: Unable to find time bounds or total distance in fit file session c:\\Users\\matth\\Documents\\Python\\CS3200\\CS3200_Strava_Secretary\\data\\export_activities\\export_96589216\\7968742010.fit\n",
      "Done loading in all users!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matth\\AppData\\Local\\Temp\\ipykernel_19028\\2876865893.py:75: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  global_challenges_csv_df = pd.read_csv(os.path.join(self.__export_filepath, 'global_challenges.csv'),\n"
     ]
    }
   ],
   "source": [
    "secretary = Secretary(all_exports = os.path.abspath(ACTIVITY_DIR_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91026afc",
   "metadata": {},
   "source": [
    "Getting all the table data out of the `Secretary`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "22965faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matth\\AppData\\Local\\Temp\\ipykernel_19028\\2876865893.py:121: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(point_dfs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User IDs: [101635319, 148511532, 57141745, 96589216]\n",
      "Combined Follows DataFrame (Before Filtering):\n",
      "      followee_user_id follow_status  follower_user_id\n",
      "0            36064991      Accepted         101635319\n",
      "1            41868570      Accepted         101635319\n",
      "2            45050667      Accepted         101635319\n",
      "3            45711813      Accepted         101635319\n",
      "4            47388563      Accepted         101635319\n",
      "..                ...           ...               ...\n",
      "947          96589216      Accepted         148511532\n",
      "948          96589216      Accepted         150679523\n",
      "949          96589216      Accepted         153835805\n",
      "950          96589216      Accepted         158514221\n",
      "951          96589216      Accepted         159639266\n",
      "\n",
      "[952 rows x 3 columns]\n",
      "Filtered Follows DataFrame:\n",
      "      followee_user_id follow_status  follower_user_id\n",
      "10           57141745      Accepted         101635319\n",
      "23           96589216      Accepted         101635319\n",
      "39          148511532      Accepted         101635319\n",
      "61          101635319      Accepted          57141745\n",
      "85          101635319      Accepted          96589216\n",
      "138         101635319      Accepted         148511532\n",
      "154          57141745      Accepted         148511532\n",
      "162          96589216      Accepted         148511532\n",
      "165         101635319      Accepted         148511532\n",
      "178         148511532      Accepted          57141745\n",
      "186         148511532      Accepted          96589216\n",
      "189         148511532      Accepted         101635319\n",
      "403          96589216      Accepted          57141745\n",
      "409         101635319      Accepted          57141745\n",
      "444         148511532      Accepted          57141745\n",
      "642          57141745      Accepted          96589216\n",
      "654          57141745      Accepted         101635319\n",
      "713          57141745      Accepted         148511532\n",
      "763          57141745      Accepted          96589216\n",
      "794         101635319      Accepted          96589216\n",
      "814         148511532      Accepted          96589216\n",
      "864          96589216      Accepted          57141745\n",
      "911          96589216      Accepted         101635319\n",
      "947          96589216      Accepted         148511532\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(OUTPUT_DIR_PATH, exist_ok=True)\n",
    "secretary.export_all_to_csvs(os.path.join('..', 'data', 'outputs'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
